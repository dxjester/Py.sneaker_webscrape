{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNEAKER WEBSITE SCRAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PURPOSE: The purpose of this program is aimed to conduct analytics of across various sneaker oriented websites\n",
    "\n",
    "AUTHOR: dxjester\n",
    "\n",
    "DATE UPDATED: 28-MAY-21"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "display(Image(\"nb_ifrit.jpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 1: ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92e84ac1d3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import time as t\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pprint\n",
    "from datetime import date \n",
    "\n",
    "from bs4 import BeautifulSoup # to parse web page data\n",
    "import glob # to read in files\n",
    "\n",
    "# plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as po\n",
    "\n",
    "# supervised learning modules\n",
    "import statistics as stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# external python files\n",
    "import plot_functions as pf\n",
    "import text_functions as tf\n",
    "# from collections import Counter\n",
    "# from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start time to calculate program duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "program_start = t.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class in order to extract and create website objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.1 Class Declaration ------------------------------------------------------#\n",
    "class sneaker_site:\n",
    "    '''\n",
    "    DESCRIPTION: Purpose of this class is to store website data located from various\n",
    "        sneaker websites and retrieve pertinent key words from each object's scrape.\n",
    "        The data scraped is then transformed into a tibble, which is then exported as \n",
    "        it's on individual CSV, later utilized for follow-on analytics\n",
    "    '''\n",
    "    \n",
    "    # initialize the class\n",
    "    def __init__ (self, name, url): # provide the name of the website and the url\n",
    "        '''\n",
    "        DESCRIPTION: initialize class with default class arguments\n",
    "        '''\n",
    "        self.website_name = name # set the name\n",
    "\n",
    "        self.url = url # save the url\n",
    "        self.site_text = '' # value to save the site text for each object\n",
    "        self.converted_site_text = '' # converting the extracted value to lower case, via the 'text_functions' file\n",
    "        self.lines = ''\n",
    "        \n",
    "        # create a dataframe to store extracted values for each object\n",
    "        self.site_df = pd.DataFrame(columns = ['website','dtg', 'date','year', 'month', 'day', 'shoe_company', 'brand', 'count'])\n",
    "        self.site_df['website'] = self.website_name # assign the website name to the entire class dataframe\n",
    "        \n",
    "        # creating Beautiful Soup variables to store individual values\n",
    "        self.soup = '' # variable to store the complete values \n",
    "        self.hyperlink_list = '' # variable to store the hyperlinks tags\n",
    "        self.paragraph_list = '' # variable to store paragraph value tags\n",
    "        self.bold_list = '' # variable to store bold value tags\n",
    "        \n",
    "        # create the site variables to aggregate total counts for each object\n",
    "        self.nike_site_count = 0\n",
    "        self.adidas_site_count = 0\n",
    "        self.reebok_site_count = 0\n",
    "        self.new_balance_site_count = 0\n",
    "        self.puma_site_count = 0\n",
    "        self.vans_site_count = 0\n",
    "\n",
    "        # default Nike list with different Nike shoe companies\n",
    "        self.nike_master = ['nike', 'jordan', 'converse'] \n",
    "        # ['Nike', 'Air', 'Max', 'Jordan', 'Zoom', 'React', 'Shox', 'ACG', 'Max Plus', 'Joyride', 'Tinker', 'Force', 'Westbrook', 'Kyrie','Lebron', 'Durant', 'SB', 'Air Max 90', 'Air Max 97', 'Air Max 1', 'Kyrie', 'Air Max 270', 'Travis Scott' ]\n",
    "\n",
    "        # default Adidas list with different Adidas shoe companies\n",
    "        self.adidas_master = ['adidas', 'reebok', 'adidas', 'kanye', 'yeezy']\n",
    "        # ['Adidas', 'ADIDAS', 'adidas', 'Yeezy', 'Kanye', 'Ultraboost', 'EQT', 'NMD', 'Ultra Boost', 'FYW', 'Harden']\n",
    "        \n",
    "        # default New Balance list \n",
    "        self.new_balance_master = ['NB', 'new balance']\n",
    "        # ['New Balance', 'NB', 'Balance', '997', '801']\n",
    "        \n",
    "        # default Puma LIst\n",
    "        self.puma_master = ['Puma', 'puma']\n",
    "        #['Puma', 'Cell Venom', 'Thunder Spectre', 'Clyde Court']\n",
    "\n",
    "        # default Vans list\n",
    "        self.vans_master = ['Vans','vans']\n",
    "        \n",
    "        # concatenante the individual sneaker lists into one master list\n",
    "        self.sneaker_list = self.nike_master + self.adidas_master + self.new_balance_master + self.puma_master + self.vans_master\n",
    "        self.length = len(self.sneaker_list)         \n",
    "        print(\"{} website object created\".format(self.website_name))\n",
    "    \n",
    "    # class function to calculate the counts of each sneaker value in the master 'sneaker_list' data structure\n",
    "    def site_calculate(self):\n",
    "        '''\n",
    "        DESCRIPTION: extract each website's raw data and append in the object's dataframe\n",
    "        '''\n",
    "        \n",
    "        # to calculate the time needed to process the function from start to finish\n",
    "        start_time = t.time() \n",
    "        print(\"\\nRetrieving {} text and data ...\".format(self.website_name))\n",
    "        \n",
    "        # establish connection to the website\n",
    "        r = requests.get(self.url)\n",
    "        self.soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        # find and categorize all hyperlink (a), paragraph (p), and bold (b) html tags\n",
    "        print(\"\\nConsolidating all hyperlinks and paragraphs for\", self.website_name)        \n",
    "        self.hyperlink_list = self.soup.findAll('a')\n",
    "        self.paragraph_list = self.soup.findAll('p')\n",
    "        self.bold_list = self.soup.findAll('b')\n",
    "        \n",
    "        # convert individual Soup categories to text\n",
    "        self.site_text = self.soup.get_text()\n",
    "        self.converted_site_text = tf.normalize_string(self.site_text)\n",
    "        print(\"\\nConverting \", self.website_name, \" to text file ... \")\n",
    "        \n",
    "        self.lines = [self.site_text.lower() for line in self.site_text]\n",
    "        print(\"\\nCalculating individual counts: \" )\n",
    "        \n",
    "        index_num = 0\n",
    "\n",
    "        # utilize the for loop to iterate over each object and count the .... \n",
    "        # ... amount of times a value is depicted in each extraction\n",
    "        for brand in self.sneaker_list:\n",
    "            \n",
    "            # allocate object variables as values for the class dataframe\n",
    "            website = self.website_name\n",
    "            name = brand + ': '\n",
    "            count = self.converted_site_text.count(brand) # count text items\n",
    "            today = date.today()\n",
    "            dtg = datetime.datetime.now()\n",
    "            year = dtg.year\n",
    "            month = dtg.month\n",
    "            day_num = dtg.day\n",
    "\n",
    "            shoe_company = ''\n",
    "            \n",
    "            # if count > 0 , aggregate the count based on shoe company name\n",
    "            if count > 0:\n",
    "                if brand in self.nike_master:\n",
    "                    self.nike_site_count += count\n",
    "                    shoe_company = 'Nike'\n",
    "                elif brand in self.adidas_master:\n",
    "                    self.adidas_site_count += count\n",
    "                    shoe_company = 'Adidas'\n",
    "                elif brand in self.new_balance_master:\n",
    "                    self.new_balance_site_count += count\n",
    "                    shoe_company = 'New Balance'\n",
    "                elif brand in self.puma_master:\n",
    "                    self.puma_site_count += count\n",
    "                    shoe_company = 'Puma'\n",
    "                elif brand in self.vans_master:\n",
    "                    self.vans_site_count += count\n",
    "                    shoe_company = 'Vans'\n",
    "                else: \n",
    "                    0\n",
    "            else: \n",
    "                if brand in self.nike_master:\n",
    "                    shoe_company = 'Nike'\n",
    "                elif brand in self.adidas_master:\n",
    "                    shoe_company = 'Adidas'\n",
    "                elif brand in self.new_balance_master:\n",
    "                    shoe_company = 'New Balance'\n",
    "                elif brand in self.puma_master:\n",
    "                    shoe_company = 'Puma'\n",
    "                elif brand in self.vans_master:\n",
    "                    shoe_company = 'Vans'\n",
    "                else: \n",
    "                    0      \n",
    "                    \n",
    "            # append each new row to the class dataframe\n",
    "            self.site_df.loc[index_num] = [website, dtg, today, year, month, day_num, shoe_company, brand, count]        \n",
    "            print(name, count)\n",
    "            index_num += 1\n",
    "        \n",
    "        elapsed_time = t.time() - start_time \n",
    "        print(\"\\n{} data ingest completed, total elapsed time: {} seconds\\n\".format(self.website_name, round(elapsed_time,2)))\n",
    "        \n",
    "    def display_info(self):\n",
    "        '''\n",
    "        DESCRIPTION: display object information\n",
    "        '''\n",
    "        print(\"\\nCalculating total counts by shoe company...\")\n",
    "        print(\"Total Nike mentions: \", self.nike_site_count)\n",
    "        print(\"Total Adidas mentions: \", self.adidas_site_count)\n",
    "        print(\"Total New Balance mentions: \", self.new_balance_site_count)\n",
    "        print(\"Total Puma mentions: \", self.puma_site_count)      \n",
    "        print(\"Total Vans mentions: \", self.vans_site_count)      \n",
    "        # print(self.site_df)      \n",
    "        \n",
    "    def return_df(self):\n",
    "        '''\n",
    "        DESCRIPTION: return class dataframe \n",
    "        '''\n",
    "        return self.site_df\n",
    "\n",
    "    def display_soup(self):\n",
    "        '''\n",
    "        DESCRIPTION: display hyperlinks for the object\n",
    "        '''\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(self.soup)\n",
    "    \n",
    "    def display_links(self):\n",
    "        '''\n",
    "        DESCRIPTION: display hyperlinks for the object\n",
    "        '''\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(self.hyperlink_list)\n",
    "    \n",
    "    def display_paragraphs(self):\n",
    "        '''\n",
    "        DESCRIPTION: display paragraphs for the object\n",
    "        '''\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(self.paragraph_list)\n",
    "        \n",
    "    def display_bold(self):\n",
    "        '''\n",
    "        DESCRIPTION: display bold tags for the object\n",
    "        '''\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(self.bold_list)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cummulative sum detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cusum(df, time_col, val_col, running_avg_count, confidence_interval):\n",
    "    '''\n",
    "    Purpose: A timeseries function aimed to conduct a change point detection analysis of timeseries data\n",
    "    '''\n",
    "    \n",
    "    mod_df = df[[time_col, val_col]]\n",
    "    mod_df['cusum'] = mod_df[val_col].cusum()\n",
    "    return mod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 2: DATA ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sneakernews.com Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 2.1: SNEAKERNEWS.com ingest and analysis -----------------------------------#\n",
    "\n",
    "start_time = t.time() # calculate elapsed time\n",
    "\n",
    "sneaker_news = sneaker_site('sneakernews.com', 'https://sneakernews.com/')\n",
    "sneaker_news.site_calculate()\n",
    "sneaker_news.display_info()\n",
    "\n",
    "elapsed_time = round(t.time() - start_time, 2)\n",
    "print(\" Total elapsed time in seconds: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the extracted site information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sneaker_news.display_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the corresponding sneakernews.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sneaker_news.display_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sneakernews.com paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sneaker_news.display_paragraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sneaker_news.display_bold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sneakernews.com dataframe from the extracted Beautiful Soup information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve master sneakernews.com dataframe\n",
    "sneaker_news_df = sneaker_news.return_df()\n",
    "sneaker_news_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot total shoe company counts for sneakernews.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# website plotting\n",
    "#pf.bar_chart(sneaker_news_df,'shoe_company', 'count', 'Sneakernews.com Count Summary')\n",
    "\n",
    "sneaker_news_raw = sneaker_news_df[['shoe_company', 'count']]\n",
    "sneaker_news_sum = sneaker_news_raw.groupby(['shoe_company']).sum().reset_index()\n",
    "\n",
    "fig = px.bar(sneaker_news_df, x='shoe_company', y='count', color = 'shoe_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the percentage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pie chart\n",
    "\n",
    "fig = px.pie(sneaker_news_df, values='count', names='shoe_company')\n",
    "fig.update_layout(\n",
    "    title=\"sneakernews.com Shoe Company Mentions \",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solecollector.com Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the solecollector.com object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 2.2: SOLECOLLECTOR.com ingest and analysis ---------------------------------#\n",
    "\n",
    "start_time = t.time() # calculate elapsed time\n",
    "\n",
    "sole_collector = sneaker_site('Solecollector.com', 'https://solecollector.com/')\n",
    "sole_collector.site_calculate()\n",
    "sole_collector.display_info()\n",
    "\n",
    "elapsed_time = round(t.time() - start_time, 2)\n",
    "print(\" Total elapsed time in seconds: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the exctracted site information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sole_collector.display_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the solecollector.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sole_collector.display_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display solecollector.com paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sole_collector.display_paragraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sole_collector.display_bold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a solecollector.com dataframe for the extracted object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve master sneakernews.com dataframe\n",
    "sole_collector_df = sole_collector.return_df()\n",
    "sole_collector_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a solecollector.com bar graph of shoe company mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# website plotting\n",
    "#pf.bar_chart(sole_collector_df,'shoe_company', 'count', 'Solecollector.com')\n",
    "\n",
    "sole_collector_raw = sole_collector_df[['shoe_company', 'count']]\n",
    "sole_collector_final = sole_collector_raw.groupby(['shoe_company']).sum().reset_index()\n",
    "\n",
    "fig = px.bar(sole_collector_final, x='shoe_company', y='count', color = 'shoe_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the percentage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pf.pie_chart(sole_collector_df,'shoe_company', 'count', 'Solecollector.com')\n",
    "\n",
    "fig = px.pie(sole_collector_df, values='count', names='shoe_company')\n",
    "fig.update_layout(\n",
    "    title=\"solecollector.com Shoe Company Mentions \",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypebeast.com Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hypebeast.com object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.3: HYPEBEAST.com ingest and analysis -------------------------------------#\n",
    "\n",
    "start_time = t.time() # calculate start time\n",
    "\n",
    "hypebeast = sneaker_site('hypebeast.com', 'https://hypebeast.com/')\n",
    "hypebeast.site_calculate()\n",
    "hypebeast.display_info()\n",
    "\n",
    "elapsed_time = round(t.time() - start_time, 2)\n",
    "print(\" Total elapsed time in seconds: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display all extracted raw hypebeast.com information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypebeast.display_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the hypebeast.com links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypebeast.display_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display hypebeast.com paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hypebeast.display_paragraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hypebeast.display_bold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hypebeast.com dataframe from the raw data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# retrieve master hypebeast.com dataframe\n",
    "hypebeast_df = hypebeast.return_df()\n",
    "hypebeast_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the shoe company mentions for hypebeast.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# website plotting\n",
    "#pf.bar_chart(hypebeast_df,'shoe_company', 'count', 'Hypebeast.com')\n",
    "\n",
    "hypebeast_raw = hypebeast_df[['shoe_company', 'count']]\n",
    "hypebeast_final = hypebeast_raw.groupby(['shoe_company']).sum().reset_index()\n",
    "\n",
    "fig = px.bar(hypebeast_final, x='shoe_company', y='count', color = 'shoe_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the percentage breakdown for hypebeast.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pf.pie_chart(hypebeast_df,'shoe_company', 'count', 'Hypebeast.com')\n",
    "\n",
    "\n",
    "df = px.data.tips()\n",
    "fig = px.pie(hypebeast_df, values='count', names='shoe_company')\n",
    "fig.update_layout(\n",
    "    title=\"Hypebeast.com Shoe Company Mentions \",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "print(\"\\n End of Phase 2 ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 3: MACRO LEVEL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this phase is to conduct a top level analysis of all cummulative data for the day executed.  The program concats three (3) x separate data frames into master dataframe, day_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sneaker_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the three individual class objects (sneaker_news_df, sole_collector_df, hypebeast_df) as a master dataframe, day_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n Starting Phase 3 ...\\n\")\n",
    "\n",
    "# concat the three dataframes into a single, unified dataframe\n",
    "frames = [sneaker_news_df, sole_collector_df, hypebeast_df]\n",
    "day_master = pd.concat(frames)\n",
    "day_master['short_date'] = day_master['dtg'].dt.date\n",
    "\n",
    "day_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the consolidated shoe company mention count for all three sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pf.bar_chart(day_master,'shoe_company', 'count', 'Consolidated Bar Chart Report')\n",
    "\n",
    "day_raw = day_master[['shoe_company', 'count']]\n",
    "day_df = day_raw.groupby(['shoe_company']).sum().reset_index()\n",
    "\n",
    "fig = px.bar(day_df, x='shoe_company', y='count', color = 'shoe_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the percentage breakdown for the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pf.pie_chart(day_master,'shoe_company', 'count', 'Consolidated Pie Report')\n",
    "\n",
    "df = px.data.tips()\n",
    "fig = px.pie(day_master, values='count', names='shoe_company')\n",
    "fig.update_layout(\n",
    "    title=\"Shoe Company Mentions (Cummulative) \",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the daily file to the root storage folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/patrickbenitez/Desktop/GT/Codebook/Git/Py.sneakernews.webscrape/df_exports/'\n",
    "# Converting date into DD-MM-YYYY format\n",
    "temp_date = datetime.datetime.today()\n",
    "\n",
    "file_date = temp_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the full file path\n",
    "full_path = path + \"v3_\" +  file_date + \".csv\"\n",
    "\n",
    "# export the file to the /df_exports/ directory\n",
    "day_master.to_csv(full_path)\n",
    "\n",
    "print(\"\\nFile successfully exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize daily counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "day_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 4: CSV IMPORT AND EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this phase is to conduct historical level analysis of all cummulative data extracted since project inception.  The program invokes the 'glob' module in order to import all standalone csv files from previous daily extracts.\n",
    "\n",
    "Due to continuous improvements in the master branch, historical data is formatted in three (3) x separate versions. The glob function imports these three (3) x separate versions for all stored files and conditions the data in order to conduct unified analysis on one master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 4.1: Determine all version 1.0 files located in the 'df_exports' directory--#\n",
    "import glob # to read in multiple csv files\n",
    "\n",
    "\n",
    "print(\"\\nRetrieving version 1.0 csv files ...\")\n",
    "\n",
    "csv_list = [] # store values in the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import version 1.0 files\n",
    "for csv_file_v1 in glob.glob('df_exports/v1_*.csv'): # only retrieve \"v1_\" csv files\n",
    "    csv_list.append(csv_file_v1)\n",
    "    print (csv_file_v1)\n",
    "    \n",
    "print(\"\\nTotal amount of v1.0 files: {}\".format(len(csv_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import version 2.0 files\n",
    "for csv_file_v2 in glob.glob('df_exports/v2_*.csv'): # only retrieve \"v2_\" csv files\n",
    "    csv_list.append(csv_file_v2)\n",
    "    print (csv_file_v2)\n",
    "\n",
    "print(\"\\nTotal amount of v2.0 files: {}\".format(len(csv_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe of version 1.0 and 2.0 files in order to change 'category_name' and 'item' column headers to 'shoe_company' and 'brand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.2: Read in each csv file into the master dataframe -----------------------#\n",
    "# 4.2.1: read in the local files and aggregate as a single dataframe -#\n",
    "old_df = pd.DataFrame(columns=['date', 'category_name', 'item', 'count'])\n",
    "\n",
    "# extract the four columns from each csv file and append to 'master_df'\n",
    "for csv_file in csv_list:\n",
    "    temp_df = pd.read_csv(csv_file)\n",
    "    sliced_df = temp_df[['date', 'category_name', 'item', 'count']]\n",
    "    old_df = pd.concat([old_df, sliced_df])\n",
    "    \n",
    "old_df['count'] = old_df['count'].astype(int)\n",
    "old_df['date'] = old_df['date'].astype('datetime64[ns]')\n",
    "old_df.rename(columns = {'category_name':'shoe_company', 'item':'brand'}, inplace = True) \n",
    "old_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import version 3.0 files\n",
    "\n",
    "csv_list2 = [] # list to store csv version 3 files\n",
    "for csv_file_v3 in glob.glob('df_exports/v3_*.csv'): # only retrieve \"v2_\" csv files\n",
    "    csv_list2.append(csv_file_v3)\n",
    "    print (csv_file_v3)\n",
    "\n",
    "print(\"\\nTotal amount of files: {}\".format(len(csv_list2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.2: Read in each csv file into the master dataframe -----------------------#\n",
    "# 4.2.1: read in the local files and aggregate as a single dataframe -#\n",
    "master_df = pd.DataFrame(columns=['date', 'shoe_company', 'brand', 'count'])\n",
    "\n",
    "# extract the four columns from each csv file and append to 'master_df'\n",
    "for csv_file in csv_list2:\n",
    "    temp_df = pd.read_csv(csv_file)\n",
    "    sliced_df = temp_df[['date', 'shoe_company', 'brand', 'count']]\n",
    "    master_df = pd.concat([master_df, sliced_df])\n",
    "    \n",
    "master_df['count'] = master_df['count'].astype(int)\n",
    "master_df['date'] = master_df['date'].astype('datetime64[ns]')\n",
    "master_df.dtypes\n",
    "master_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df = old_df.append(master_df)\n",
    "final_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the histogram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fd99596318a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# group by sum the master_df dataframe for follow-on analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaster_sum_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'shoe_company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brand'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msummarized_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_sum_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaster_sum_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummarized_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# group by sum the master_df dataframe for follow-on analysis\n",
    "master_sum_df = final_df.groupby(['date','shoe_company', 'brand']).sum().reset_index()\n",
    "summarized_df = master_sum_df[master_sum_df['count'] != 0]\n",
    "summarized_df.head(5)\n",
    "\n",
    "sns.pairplot(summarized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_sum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the 'brand' column from the master_sum_df and name new df as 'category_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.2.2: Unstack and pairplot the master dataframe for category_name df - #\n",
    "category_df = master_sum_df[['date','shoe_company','count']]\n",
    "category_df = category_df.groupby(['date','shoe_company']).sum().reset_index()\n",
    "category_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstack the 'shoe_company' values as stand alone columns in order to conduct multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unstack_category_df = category_df.pivot_table(index = ['date'], \n",
    "                                   columns = 'shoe_company',\n",
    "                                   values = 'count',\n",
    "                                   aggfunc='first').reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "unstack_category_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot the 'unstack_category_df' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(unstack_category_df) # pairplot the category dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe with 'brand' and 'date' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "item_temp_df = master_df[['date','brand','count']]\n",
    "item2_df = item_temp_df.groupby(['date', 'brand']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows where the count is equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove rows where count is equal to '0'\n",
    "item_df = item2_df[item2_df['count'] != 0]\n",
    "item_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstack the 'item_df' dataframe in order to conduct follow-on multivariate regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unstack_item_df = item_df.pivot_table(index = ['date'],\n",
    "                                      columns = 'brand',\n",
    "                                      values = 'count',\n",
    "                                      aggfunc='first').reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "unstack_item_df.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot the 'unstack_item_df' dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.pairplot(unstack_item_df) # pairplot the item dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a count for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b232252fb2ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4.2.4: Date and Count dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdate_count_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdate_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_count_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdate_count_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 4.2.4: Date and Count dataframe\n",
    "date_count_temp = final_df[['date','count']]\n",
    "date_count_df = date_count_temp.groupby('date').sum().reset_index()\n",
    "date_count_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a timeseries plot of all counts by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# master counts, company agnostic\n",
    "\n",
    "fig = px.line(date_count_df, x='date', y='count')\n",
    "fig.update_layout(\n",
    "    title=\"Shoe Company Daily Count Summary\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Total Daily Counts\",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create timeseries analysis, categorized by shoe company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# master counts, by day and shoe company\n",
    "\n",
    "# 4.2.4: Date and Count dataframe\n",
    "date_shoe_temp = final_df[['date','shoe_company', 'count']]\n",
    "date_shoe_df = date_shoe_temp.groupby(['date', 'shoe_company']).sum().reset_index()\n",
    "\n",
    "fig = px.line(date_shoe_df, x='date', y='count', color='shoe_company')\n",
    "fig.update_layout(\n",
    "    title=\"Shoe Company Daily Count Summary\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Total Daily Counts\",\n",
    "    font=dict(\n",
    "        family=\"Helvetica\",\n",
    "        size=12,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the area chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-11007177232c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_shoe_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'shoe_company'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "fig = px.area(date_shoe_df, x='date', y='count', color = 'shoe_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sankey chart in order to depict feeder flow from shoe company to brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_df = master_df.groupby(['date', 'shoe_company', 'brand']).sum().reset_index()\n",
    "\n",
    "fig = pf.genSankey(sum_df,cat_cols=['shoe_company', 'brand'],value_cols='count',title='Sneaker Sankey Analysis')\n",
    "po.offline.plot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice dataframe in order to begin multivariate regression.\n",
    "\n",
    "Set the shoe companie column values as the predictor variables, with 'total counts' serving as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unstack_df = unstack_category_df.copy()\n",
    "unstack_df.fillna(0, inplace = True)\n",
    "unstack_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column of total counts, categorized by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unstack_df['total_counts'] = unstack_df['Adidas']  + unstack_df['New Balance'] + unstack_df['Nike'] + unstack_df['Puma'] + unstack_df['Vans']\n",
    "\n",
    "unstack_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = unstack_df[['Adidas', 'New Balance','Nike','Puma','Vans']]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the response variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unstack_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f12c96fc624d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munstack_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unstack_df' is not defined"
     ]
    }
   ],
   "source": [
    "y = unstack_df[['total_counts']]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model boject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Intercept: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Coefficients: \\n', regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and predict values for each of the given records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with statsmodels\n",
    "X = sm.add_constant(X) # adding a constant\n",
    " \n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a three-dimensional plot of Nike vs. Adidas values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice out Nike and Adidas counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tri_dim = unstack_df[['Adidas','Nike','total_counts']]\n",
    "tri_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 95% confidence interval for total_counts column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f24f3f8dbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtri_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy.stats as st\n",
    "\n",
    "count_list = list(tri_dim['total_counts'])\n",
    "count_array = np.array(count_list)\n",
    "\n",
    "lower, higher = st.t.interval(0.95, len(count_array)-1, loc=np.mean(count_array), scale=st.sem(count_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_mask = (tri_dim['total_counts'] > lower) & (tri_dim['total_counts'] < higher)\n",
    "tri_mask = tri_dim.loc[count_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D plot Nike vs. Adidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "threedee = plt.figure(figsize = (12,10)).gca(projection='3d')\n",
    "threedee.scatter(tri_mask['Adidas'], tri_mask['Nike'], tri_mask['total_counts'], cmap=cm.rainbow)\n",
    "plt.title('Adidas vs Nike counts')\n",
    "threedee.set_xlabel('Adidas')\n",
    "threedee.set_ylabel('Nike')\n",
    "threedee.set_zlabel('Total Counts')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Point Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve data from the unstacked dataframe, previously aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unstack_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice on the 'date,' 'Nike,' and 'total_counts' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cusum_nike_raw = unstack_df[['date', 'Nike']]\n",
    "cusum_nike_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the cummulative sum for the Nike column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cusum_nike_raw['cusum'] = cusum_nike_raw['Nike'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the Nike data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cusum_nike_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate running 3 day average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,cusum_nike_raw.shape[0]-2):\n",
    "    cusum_nike_raw.loc[cusum_nike_raw.index[i+2],'Nike SMA_3'] = np.round(((cusum_nike_raw.iloc[i,1]+ cusum_nike_raw.iloc[i+1,1] + cusum_nike_raw.iloc[i+2,1])/3),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top 15 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cusum_nike_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5c862c9351c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcusum_nike_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cusum_nike_raw' is not defined"
     ]
    }
   ],
   "source": [
    "cusum_nike_raw.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate rolling standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cusum_nike_raw['sigma_3day'] = cusum_nike_raw['Nike'].rolling(3).std()\n",
    "cusum_nike_raw['sigma_7day'] = cusum_nike_raw['Nike'].rolling(7).std()\n",
    "cusum_nike_raw.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conver the cummulative column to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nike_count_list = list(cusum_nike_raw['Nike'])\n",
    "nike_count_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the five number summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Minimum value: \", np.min(nike_count_list))\n",
    "print(\"Maximum value: \", np.max(nike_count_list))\n",
    "print(\"Standard Deviation: \", np.std(nike_count_list))\n",
    "print(\"Mean: \", np.mean(nike_count_list))\n",
    "print(\"Median: \", np.median(nike_count_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(nike_count_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create five number summary of the running 3 day standard deviation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "ma_count_list = list(cusum_nike_raw['Nike SMA_3'])\n",
    "cleaned_ma3_list = [0.0 if math.isnan(x) else x for x in ma_count_list]\n",
    "cleaned_ma3_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Minimum value: \", np.min(cleaned_ma3_list))\n",
    "print(\"Maximum value: \", np.max(cleaned_ma3_list))\n",
    "print(\"Standard Deviation: \", np.std(cleaned_ma3_list))\n",
    "print(\"Mean: \", np.mean(cleaned_ma3_list))\n",
    "print(\"Median: \", np.median(cleaned_ma3_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(cleaned_ma3_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create five number summary of the running 7 day standard deviation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ma_count_list = list(cusum_nike_raw['sigma_7day'])\n",
    "cleaned_ma7_list = [0.0 if math.isnan(x) else x for x in ma_count_list]\n",
    "cleaned_ma7_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Minimum value: \", np.min(cleaned_ma7_list))\n",
    "print(\"Maximum value: \", np.max(cleaned_ma7_list))\n",
    "print(\"Standard Deviation: \", np.std(cleaned_ma7_list))\n",
    "print(\"Mean: \", np.mean(cleaned_ma7_list))\n",
    "print(\"Median: \", np.median(cleaned_ma7_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(cleaned_ma7_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR Model Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a timeseries forecasting analysis using the ARIMA model, using the Nike dataframe and counts as a base testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a65eb7c55e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice a new dataframe with the Nike values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nike_forecast = cusum_nike_raw[['date','Nike']]\n",
    "nike_forecast.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the datatypes of the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nike_forecast.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "series = [i+randrange(10) for i in range(1,100)]\n",
    "series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice out cummulative data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nike_forecast = cusum_nike_raw[['date','Nike']]\n",
    "nike_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the 'date' column as the index for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nike_forecast.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sliced dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nike_forecast.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the auto correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(nike_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the differences between individual dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nike_diff = nike_forecast.diff(periods = 1)\n",
    "nike_diff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nike_diff.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nike_diff.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the autocorrelation for the nike difference dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_acf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a402a671ef18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_acf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnike_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_acf' is not defined"
     ]
    }
   ],
   "source": [
    "plot_acf(nike_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Nike values to a numpy array for calcualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = nike_forecast[['Nike']].to_numpy()\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = X[:80]\n",
    "train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = X[81:]\n",
    "test.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predictions list to store the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the AR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ar = AR(train)\n",
    "model_ar_fit = model_ar.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the values from index 80 to 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model_ar_fit.predict(start = 80, end= 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.plot(nike_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Model Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the ARIMA model function in order to build the forecast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters: p , d, q\n",
    "# p = periods taken for autoregessive model\n",
    "# d = order of integrated, number of times differences is executed\n",
    "# q = periods in moving average model\n",
    "\n",
    "model_arima = ARIMA(train, order = (10,0,1) )\n",
    "model_arima_fit = model_arima.fit()\n",
    "print(model_arima_fit.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arima_predictions = model_arima_fit.forecast(steps = 25)[0] # predict 25 values\n",
    "arima_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the forecasted data against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.title(\"Arima Forecasting Analysis\")\n",
    "plt.plot(test)\n",
    "plt.plot(arima_predictions, color ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_arima_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC Value Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the AIC value in order to identify the optimized pdq set for the optimized model.  Lowest value is taken and inputted back in the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (0, 0, 2),\n",
       " (0, 0, 3),\n",
       " (0, 0, 4),\n",
       " (0, 1, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 2),\n",
       " (0, 1, 3),\n",
       " (0, 1, 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "p=d=q=range(0,5)\n",
    "pdq = list(itertools.product(p,d,q))\n",
    "pdq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for parameter in pdq:\n",
    "    try:\n",
    "        model_arima = ARIMA(train, order = parameter )\n",
    "        model_arima_fit = model_arima.fit()\n",
    "        print(parameter, model_arima_fit.aic)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cusum_nike_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2530c6e5b809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnike_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcusum_nike_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nike'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnike_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cusum_nike_raw' is not defined"
     ]
    }
   ],
   "source": [
    "nike_count = cusum_nike_raw[['Nike']]\n",
    "nike_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(len(nike_count.Nike)*.8)\n",
    "test_length = len(nike_count) - train_length\n",
    "\n",
    "print(\"Train length {}\".format(train_length))\n",
    "print(\"Test length {}\".format(test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nike_count.Nike[:train_length]\n",
    "test = nike_count.Nike[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = len(test)\n",
    "test_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc, se, conf = model_arima_fit.forecast(test_length, alpha=0.05)  # 95% conf\n",
    "fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_series = pd.Series(fc, index=test.index)\n",
    "lower_series = pd.Series(conf[:, 0], index=test.index)\n",
    "upper_series = pd.Series(conf[:, 1], index=test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the final findings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train, label='training')\n",
    "plt.plot(test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "program_end = t.time() - program_start\n",
    "print(\"Total time for program execution: \", round(program_end, 2), \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
